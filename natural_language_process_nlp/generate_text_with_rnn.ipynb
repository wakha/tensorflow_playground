{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d77ef03",
   "metadata": {},
   "source": [
    "# Generative networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short-Term Memory cells (LSTMs) and Gated Recurrent Units (GRUs) provide a mechanism for language modeling &mdash; they can learn word ordering and make predictions for the next word in a sequence. This allows us to use RNNs for **generative tasks**, such as ordinary text generation, machine translation, and even image captioning.\n",
    "\n",
    "In the RNN architecture we discussed in the previous notebook, each RNN unit produced the next hidden state as an output. However, we could also add another output to each recurrent unit, which would allow us to output a **sequence** with the same length as the original sequence. Moreover, we could use RNN units that don't accept an input at each step, instead they take an initial state vector and produce a sequence of outputs.\n",
    "\n",
    "This allows for different neural architectures, as is shown in the picture below:\n",
    "\n",
    "![Image showing common recurrent neural network patterns.](notebooks/images/unreasonable-effectiveness-of-rnn.jpg)\n",
    "*Image from blog post [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by [Andrej Karpaty](http://karpathy.github.io/)*\n",
    "\n",
    "* **One-to-one** is a traditional neural network with one input and one output.\n",
    "* **One-to-many** is a generative architecture that accepts one input value, and generates a sequence of output values. For example, if we want to train an **image captioning** network that produces a textual description of a picture, we can use a picture as input, pass it through a CNN to obtain the hidden state, and then have a recurrent chain generate the caption word-by-word.\n",
    "* **Many-to-one** corresponds to the RNN architectures we described in the previous unit, such as text classification.\n",
    "* **Many-to-many**, or **sequence-to-sequence** corresponds to tasks such as **machine translation**, in which case the RNN first collects all the information from the input sequence into the hidden state, and then unrolls this state into the output sequence.\n",
    "\n",
    "In this unit, we will focus on simple generative models that help us generate text. For simplicity, let's build a **character-level network**, which generates text letter by letter. During training, we need to take a text corpus, and split it into letter sequences. \n",
    "\n",
    "> For the sandbox environment, we need to run the following cell to make sure the required library is installed, and data is prefetched. If you're running locally, you can skip the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc004b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# In this tutorial, we will be training a lot of models. In order to use GPU memory cautiously,\n",
    "# we will set tensorflow option to grow GPU memory allocation when required.\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "if len(physical_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa7b13",
   "metadata": {},
   "source": [
    "## Building character vocabulary\n",
    "\n",
    "To build a character-level generative network, we need to split the text into individual characters instead of words. The `TextVectorization` layer that we've been using can't do that, so we have two options:\n",
    "\n",
    "* Manually load text and implement our own tokenization, as in [this official Keras example](https://keras.io/examples/generative/lstm_character_level_text_generation/).\n",
    "* Use the `Tokenizer` class for character-level tokenization.\n",
    "\n",
    "We'll go with the second option. `Tokenizer` can also be used to tokenize into words, so we should be able to switch from character-level to word-level tokenization quite easily.\n",
    "\n",
    "To do character-level tokenization, we need to pass the `char_level=True` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46994a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True,lower=False)\n",
    "tokenizer.fit_on_texts([x['title'].numpy().decode('utf-8') for x in ds_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d2212",
   "metadata": {},
   "source": [
    "We also want to use one special token to denote **end-of-sequence**, which we'll call `<eos>`. Let's add it manually to the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a385185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = len(tokenizer.word_index)+1\n",
    "tokenizer.word_index['<eos>'] = eos_token\n",
    "\n",
    "vocab_size = eos_token + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892aae76",
   "metadata": {},
   "source": [
    "Now, to encode text into sequences of numbers, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c956cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[48, 2, 10, 10, 5, 44, 1, 25, 5, 8, 10, 13, 78]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Hello, world!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71bec05",
   "metadata": {},
   "source": [
    "## Training a generative RNN to generate titles\n",
    "\n",
    "Here's how we'll train an RNN to generate titles: on each step, we'll take one title as input, and for each input character in that title, we'll train the network to generate the next character as output:\n",
    "\n",
    "![Image showing an example RNN generation of the word 'HELLO'.](notebooks/images/rnn-generate.png)\n",
    "\n",
    "For the last character of our sequence, we'll ask the network to generate the `<eos>` token.\n",
    "\n",
    "The main difference between the generative RNN we're using here and the RNNs we've seen previously is that each cell of the generative RNN will produce an output, not just the final cell. This can be achieved by specifying the `return_sequences` parameter of the RNN cell.\n",
    "\n",
    "Thus, during training, an input to the network would be a sequence of encoded characters of some length, and the corresponding output would be a sequence of the same length, but shifted by one element and terminated by `<eos>`. The minibatch will consist of several such sequences, and we'll need to use **padding** to align all sequences.\n",
    "\n",
    "Let's create functions that transform the dataset for us. Because we want to pad sequences at the minibatch level, we'll first create the minibatches by calling `.batch()`, and then use `map` to do transformation. Here's the code for the function that does the transformation, which takes a whole minibatch as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e029cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_batch(x):\n",
    "    x = [t.numpy().decode('utf-8') for t in x]\n",
    "    z = tokenizer.texts_to_sequences(x)\n",
    "    z = tf.keras.preprocessing.sequence.pad_sequences(z)\n",
    "    return tf.one_hot(z,vocab_size), tf.one_hot(tf.concat([z[:,1:],tf.constant(eos_token,shape=(len(z),1))],axis=1),vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de15ae7",
   "metadata": {},
   "source": [
    "A few important things that we do here:\n",
    "* We first extract the actual text from the string tensor.\n",
    "* `text_to_sequences` converts the list of strings into a list of integer tensors.\n",
    "* `pad_sequences` pads those tensors to their maximum length.\n",
    "* We finally one-hot encode all the characters, and also do the shifting and `<eos>` appending. We will soon see why we need one-hot-encoded characters.\n",
    "\n",
    "However, this function is **Pythonic** &mdash; it cannot be automatically translated into the TensorFlow computational graph. (Computational graphs will be discussed in more detail in the module on going beyond Keras of this learning path.) If we try to pass it directly to the `Dataset.map` function, which is how we intend to use this function, we'll get an error. To get around this problem, we need to enclose this Pythonic call by using the `py_function` wrapper: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd6dc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_batch_fn(x):\n",
    "    x = x['title']\n",
    "    a,b = tf.py_function(title_batch,inp=[x],Tout=(tf.float32,tf.float32))\n",
    "    a.set_shape((None, None, vocab_size))  # input: batch, time, vocab_size\n",
    "    b.set_shape((None, None, vocab_size))  # target: same shape\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbbc08",
   "metadata": {},
   "source": [
    "> **Note**: You may be wondering if instead of calling the `py_function` wrapper we could transform the dataset using standard Python functions before passing it to `fit`. While this can definitely be done, when using `Dataset.map` the data transformation pipeline is executed using TensorFlow's computational graph, which takes advantage of GPU computations and minimizes the need to pass data between CPU/GPU.\n",
    "\n",
    "We can now build our generator network and start training. Our network can be based on any recurrent cell which we discussed in the previous unit (simple, LSTM or GRU). In our example we'll use LSTM.\n",
    "\n",
    "Because the network takes characters as input, the vocabulary size is pretty small. Therefore we don't need an embedding layer &mdash; we can feed one-hot-encoded input directly into the LSTM cell. The output layer is then a `Dense` classifier that will convert the LSTM output into one-hot-encoded token numbers.\n",
    "\n",
    "In addition, since we're dealing with variable-length sequences, we can use a `Masking` layer to create a mask that will ignore the padded part of the string. This is not strictly needed, because we're not interested in everything that goes beyond the `<eos>` token, but we will use it to get some experience with this layer type. The input shape is `(None, vocab_size)`, where `None` indicates the sequence of variable length, and the output shape is also `(None, vocab_size)`, as you can see from the `summary` printout below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0baa948d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">109,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,836</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking_3 (\u001b[38;5;33mMasking\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m109,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)       │        \u001b[38;5;34m10,836\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">119,892</span> (468.33 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m119,892\u001b[0m (468.33 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">119,892</span> (468.33 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m119,892\u001b[0m (468.33 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15000/15000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 33ms/step - loss: 1.6305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a0f6dc49b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Masking(input_shape=(None,vocab_size)),\n",
    "    keras.layers.LSTM(128,return_sequences=True),\n",
    "    keras.layers.Dense(vocab_size,activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(ds_train.batch(8).map(title_batch_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb38c26",
   "metadata": {},
   "source": [
    "## Generating output\n",
    "\n",
    "Now that we've trained the model, we want to use it to generate output. First of all, we need a way to decode text represented by a sequence of token numbers. We could use the `tokenizer.sequences_to_texts` function, but it doesn't work well with character-level tokenization. Therefore we will take a dictionary of tokens from the tokenizer (called `word_index`), build a reverse map, and write our own decoding function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fdbf5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_map = {val:key for key, val in tokenizer.word_index.items()}\n",
    "\n",
    "def decode(x):\n",
    "    return ''.join([reverse_map.get(t,'') for t in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ee874",
   "metadata": {},
   "source": [
    "Now, let's do the generation. We first encode a string passed as parameter into a sequence, and then on each step we call our network to infer the next character. \n",
    "\n",
    "The output of the network is a vector of `vocab_size` elements representing probablities of each token, and we can find the most probably token number by using `argmax`. We then append this character to the generated list of tokens, and proceed with the generation. This process of generating one character is repeated `size` times to generate the required number of characters, and we terminate early if `eos_token` is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a90a3707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today #39;s street to street to street to start for the search in Iraq'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(model,size=100,start='Today '):\n",
    "        inp = tokenizer.texts_to_sequences([start])[0]\n",
    "        chars = inp\n",
    "        for i in range(size):\n",
    "            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
    "            nc = tf.argmax(out)\n",
    "            if nc==eos_token:\n",
    "                break\n",
    "            chars.append(nc.numpy())\n",
    "            inp = inp+[nc]\n",
    "        return decode(chars)\n",
    "    \n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d9a29",
   "metadata": {},
   "source": [
    "## Sampling output during training\n",
    "\n",
    "Because we don't have any useful metrics such as *accuracy*, the only way we can see that our model is getting better is by **sampling** generated strings during training. We use **callbacks** for that, which are functions that we can pass to the `fit` function, and that will be called periodically during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6574952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m15000/15000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.3967Today #39; to seek for the report for the report for the plane\n",
      "\u001b[1m15000/15000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 24ms/step - loss: 1.3675\n",
      "Epoch 2/3\n",
      "\u001b[1m14999/15000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.2959Today #39; market to start for the start for the start for the start &lt;b&gt;...&lt;/b&gt;\n",
      "\u001b[1m15000/15000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 26ms/step - loss: 1.2829\n",
      "Epoch 3/3\n",
      "\u001b[1m14999/15000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.2454Today #39; more to start for the street in the strike for &lt;b&gt;...&lt;/b&gt;\n",
      "\u001b[1m15000/15000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 33ms/step - loss: 1.2387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a0f2f35a00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_callback = keras.callbacks.LambdaCallback(\n",
    "  on_epoch_end = lambda batch, logs: print(generate(model))\n",
    ")\n",
    "\n",
    "model.fit(ds_train.batch(8).map(title_batch_fn),callbacks=[sampling_callback],epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2bd659",
   "metadata": {},
   "source": [
    "## Soft text generation and temperature\n",
    "\n",
    "In the code for the `generate` function, we took the character with the highest probability as the next character in the generated text. This resulted in text that cycles between the same character sequences again and again, like in this example:\n",
    "```\n",
    "today of the second the company and a second the company ...\n",
    "```\n",
    "\n",
    "However, if we look at the probability distribution for the next character, it may be that there are several high probabilities that are pretty similar. For example, when looking for the next character in the sequence '*play*', it's similarly likely that it's either space or **e** (as in the word *player*).\n",
    "\n",
    "Therefore, it's not always the best choice to select the character with the absolute highest probability &mdash; choosing the second or third highest might still lead to meaningful text, and may avoid cycling through character sequences. Therefore, a better strategy is to **sample** characters from the probability distribution given by the network output.\n",
    "\n",
    "This sampling can be done using the `np.multinomial` function which implements a **multinomial distribution**. A function that implements this **soft** text generation is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfd90a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature = 0.3\n",
      "Today to company seen at game for head to stand in Australia to say &lt;b&gt;...&lt;/b&gt;\n",
      "On Sunday B Seen Card Deal of Carter (Reuters)\n",
      "Moscow, S report to security case to be surgering with return\n",
      "President corting to service for Marsh profit of partnership (AFP)\n",
      "Little red riding hood ting for security for a the market\n",
      "\n",
      "--- Temperature = 0.8\n",
      "Today sident warning suspects Profit offrust computer strategy\n",
      "On Sunday FI Slights Trade to Conte Bush for Sales (Reuters)\n",
      "Moscow, EGO Brazil Set for Propotiem (AP)\n",
      "President odnet to defending into 450 patents blait at rouble (UKTapa)\n",
      "Little red riding hood ole with seek #39; peace triath dead\n",
      "\n",
      "--- Temperature = 1.0\n",
      "Today Gattled To Place from Notable at Nec! In Based (Reuters)\n",
      "On Sunday freled Menterces evento poteni stronlite (USATODEYY.con)\n",
      "Moscow, IG  #39;HP #39; in Iraq's, ICabosia pusheder\n",
      "President fral news crad to the new phone testives  &xt.c. /SM\n",
      "Little red riding hood fo net the storm to stendarly in nut Forw GM (AFP)\n",
      "\n",
      "--- Temperature = 1.3\n",
      "Today annsprest a dictionabers on bib #39;s plughers bone videops\n",
      "On Sunday A Nucleisile coacher, Japan claims barligal\n",
      "Moscow, CO May  #elearsn of #39;thQ #39;  #39;Crade neven figitht #39;\n",
      "President oles study #36; cuspave EODMA4d\n",
      "Little red riding hood b CMs Stumpingtorma lewald 9WG vot hos?igger\n",
      "\n",
      "--- Temperature = 1.8\n",
      "Today sBoclO7 E cealhgragersit, Gint: Korea &lotgaasPo)Wojat \\$15kbohn\n",
      "On Sunday s Knaz inqurery prissions,, recsondermy, elevisuht\n",
      "Moscow, NIt res=wie, Wep):, Austratio on WrCt's-veebes \\LQ (AFP)\n",
      "President Imy, OrtBA\n",
      "Little red riding hood 05d fo-With MiPo TP EunoP Intels\n"
     ]
    }
   ],
   "source": [
    "def generate_soft(model,size=100,start='Today ',temperature=1.0):\n",
    "        inp = tokenizer.texts_to_sequences([start])[0]\n",
    "        chars = inp\n",
    "        for i in range(size):\n",
    "            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
    "            probs = tf.exp(tf.math.log(out)/temperature).numpy().astype(np.float64)\n",
    "            probs = probs/np.sum(probs)\n",
    "            nc = np.argmax(np.random.multinomial(1,probs,1))\n",
    "            if nc==eos_token:\n",
    "                break\n",
    "            chars.append(nc)\n",
    "            inp = inp+[nc]\n",
    "        return decode(chars)\n",
    "\n",
    "words = ['Today ','On Sunday ','Moscow, ','President ','Little red riding hood ']\n",
    "    \n",
    "for i in [0.3,0.8,1.0,1.3,1.8]:\n",
    "    print(f\"\\n--- Temperature = {i}\")\n",
    "    for j in range(5):\n",
    "        print(generate_soft(model,size=300,start=words[j],temperature=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ded96",
   "metadata": {},
   "source": [
    "We've introduced one more parameter called **temperature**, which indicates how strongly we should choose higher probability characters over lower probability ones. If the temperature is close to 0, we choose the highest probability character, and when the temperature approaches infinity then all probabilities become equal, and we randomly select the next character. In the example above we can observe that the output becomes meaningless when we increase the temperature too much, and starts cycling when we lower it closer to 0. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
